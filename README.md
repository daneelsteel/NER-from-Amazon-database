# Извлечение именованных сущностей из отзывов на регулярные издания с Амазона

Что мы можем сделать для извлечения?

- составить шаблоны для сочетаний слов, в которых можно найти нужные НЕР. Из плюсов - легко, быстро для машинного поиска и гарантированно правила будут иметь смысл, потому что мы сами ручками их задали, глазками посмотрев на энное количество отзывов. Из минусов? На все 90 тысяч отзывов не посмотришь, а правила выделения по количеству будут быстро расти.
- пойти другим путем и составить шаблоны частей речи, чаще всего являющиеся соседями наших НЕР. Похоже на предыдущий вариант, только правил придется делать куда меньше, но отсев кандидатов придется делать значительно более жесткий
- выделить энное количество (например, сотню-две) триграмм с ближайшими контекстами для настоящих и проверенных НЕР, векторизовать и триграммы, и отдельно контексты без НЕР, сравнивать кандидатные триграммы по близости с этим золотым стандартом

Мое сердце лежит скорее к шаблонам, так что их и попробуем построить, а потом отранжируем через метрики Т, Дайса и ПМИ.

Все результаты хранятся в тетрадке ner_rules.ipynb, а скопированный датасет прикреплен в виде файла .tar.gz.

Что можно сказать по результатам? Что исправить, чтобы все стало лучше?

- для РАКЕ: добиться того, чтобы он выделял ключевые блоки с большей длиной и не крутился вечно вокруг однословных - впрочем, возможно, это у нас маленькие кусочки текстов и мало токенов; убедить его не токенизировать по дефисам и вообще дополнительно не токенизировать
- для УАКЕ: ничего, он умничка, чмок в лобик
- для textrank: узнать, что же там происходит с частеречным делением, что мешает молду фильтровать у него вечные глаголы-прилагательные; тоже как-то пихнуть его в сторону более длинных ключевых блоков - возможно, присваивать более длинным блокам чуточку большие веса? Не факт, что это технически выполнимо, и если даже и да, нужно будет очень точно определить коэффициент домножения весов, чтобы он не начал компульсивно собирать все длинные цепочки, что можно придумать.
- для всех: возможно, деление на части речи стоит производить еще когда текст не очищен и не лемматизирован. Куда-то туда можно воткнуть этот этап; впрочем, он особо не поможет, когда моделькам придется расставлять части речи своих кандидатов - у кандидатов-то контекста нет. Можно, например, для каждого кандидата искать его же в изначальном тексте содержания и вытаскивать часть речи оттуда - там у него еще есть полный контекст.
